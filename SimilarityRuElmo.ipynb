{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "teVN4SM5Shzl"
      },
      "source": [
        "import pandas as pd\n",
        "sample=pd.read_csv('busines_norm.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOVRqoR-yIXd"
      },
      "source": [
        "sample = sample[sample.norm!='-'].dropna()\n",
        "y = sample.norm\n",
        "sample = sample.drop(['Unnamed: 0', 'norm', 'text'], axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhY9_QHcS_1U"
      },
      "source": [
        "import re\n",
        "i=0;\n",
        "for sent in sample['title'].values:\n",
        "    if (len(re.findall('<.*?>', sent))):\n",
        "        print(i)\n",
        "        print(sent)\n",
        "        break;\n",
        "    i += 1;"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY8YsPZdyPNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9a1ab7-4032-4c64-a07b-555917890699"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sno = nltk.stem.SnowballStemmer('russian') #initialising the snowball stemmer which is developed in recent years\n",
        "stop=set(stopwords.words('russian'))\n",
        "\n",
        "\n",
        "\n",
        "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned\n",
        "print(stop)\n",
        "print('************************************')\n",
        "print(sno.stem('tasty'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'нет', 'всего', 'между', 'вы', 'хорошо', 'этот', 'мне', 'перед', 'сейчас', 'них', 'их', 'чтоб', 'есть', 'раз', 'были', 'можно', 'конечно', 'же', 'когда', 'от', 'во', 'себе', 'вам', 'того', 'свою', 'где', 'него', 'ж', 'ведь', 'не', 'всех', 'даже', 'под', 'по', 'ни', 'нас', 'этом', 'себя', 'такой', 'никогда', 'хоть', 'ты', 'про', 'у', 'до', 'с', 'из', 'там', 'для', 'они', 'я', 'он', 'чуть', 'тоже', 'зачем', 'наконец', 'что', 'ну', 'был', 'всегда', 'куда', 'ли', 'только', 'еще', 'разве', 'потому', 'моя', 'более', 'этого', 'им', 'об', 'да', 'три', 'лучше', 'его', 'том', 'больше', 'чего', 'надо', 'над', 'опять', 'за', 'без', 'было', 'а', 'будет', 'ей', 'здесь', 'ней', 'чтобы', 'какой', 'вас', 'совсем', 'о', 'мы', 'ему', 'к', 'много', 'почти', 'тебя', 'нее', 'чем', 'иногда', 'или', 'кто', 'нибудь', 'впрочем', 'быть', 'через', 'тогда', 'меня', 'будто', 'другой', 'потом', 'нельзя', 'эти', 'может', 'то', 'сам', 'уже', 'так', 'бы', 'в', 'после', 'теперь', 'ним', 'мой', 'но', 'все', 'при', 'вот', 'была', 'на', 'тем', 'тот', 'как', 'если', 'и', 'она', 'один', 'два', 'вдруг', 'эту', 'ее', 'тут', 'какая', 'уж', 'этой', 'ничего', 'всю', 'со'}\n",
            "************************************\n",
            "tasty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvsECav_0I9D"
      },
      "source": [
        "i=0\n",
        "str1=' '\n",
        "final_string=[]\n",
        "all_positive_words=[] # store words from +ve reviews here\n",
        "all_negative_words=[] # store words from -ve reviews here.\n",
        "s=''\n",
        "for sent in sample['title'].values:\n",
        "    filtered_sentence=[]\n",
        "    #print(sent);\n",
        "    sent=cleanhtml(sent) # remove HTMl tags\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
        "                if(cleaned_words.lower() not in stop):\n",
        "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
        "                    filtered_sentence.append(s)\n",
        "                    if (sample['title'].values)[i] == 'positive': \n",
        "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
        "                    if(sample['title'].values)[i] == 'negative':\n",
        "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue \n",
        "    #print(filtered_sentence)\n",
        "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
        "    #print(\"***********************************************************************\")\n",
        "    \n",
        "    final_string.append(str1)\n",
        "    i+=1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVkyLzUTm7m"
      },
      "source": [
        "sample['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
        "sample['CleanedText']=sample['CleanedText'].str.decode(\"utf-8\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "list_of_sent_train=[]\n",
        "for sent in sample['CleanedText'].values:\n",
        "    filtered_sentence=[]\n",
        "    sent=cleanhtml(sent)\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if(cleaned_words.isalpha()):    \n",
        "                filtered_sentence.append(cleaned_words.lower())\n",
        "            else:\n",
        "                continue \n",
        "    list_of_sent_train.append(filtered_sentence)"
      ],
      "metadata": {
        "id": "j5oOtyPButEr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "# Training the wor2vec model using train dataset\n",
        "w2v_model=gensim.models.Word2Vec(list_of_sent_train,workers=4)"
      ],
      "metadata": {
        "id": "sZj5bIl3uuV_",
        "outputId": "1a479d18-07b2-4b08-97d2-4bbf70fa4cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this train\n",
        "for sent in list_of_sent_train: # for each review/sentence\n",
        "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /= cnt_words\n",
        "    sent_vectors.append(sent_vec)\n",
        "sent_vectors = np.array(sent_vectors)\n",
        "sent_vectors = np.nan_to_num(sent_vectors)\n",
        "sent_vectors.shape"
      ],
      "metadata": {
        "id": "1m5F-3vkuuoM",
        "outputId": "9f9f6947-7d6d-4c2a-d904-a1cb8c099fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(466, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = sent_vectors"
      ],
      "metadata": {
        "id": "HNVk0nA5u9jw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)"
      ],
      "metadata": {
        "id": "qlMNShriu-Ul"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "clf=DecisionTreeClassifier()\n",
        "parametrs = {'criterion': ['gini', 'entropy', 'log_loss'], 'max_depth': range(2,5), 'min_samples_split': range(1,5), 'min_samples_leaf': range(1,5)}\n",
        "grid_clf = GridSearchCV(clf, parametrs, cv = 5)\n",
        "grid_clf.fit(x,y)\n",
        "grid_clf.best_params_"
      ],
      "metadata": {
        "id": "ltsk05jTu_WP",
        "outputId": "a1fcca51-581f-4284-fbfe-b0a40080dde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "360 fits failed out of a total of 720.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "180 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 942, in fit\n",
            "    X_idx_sorted=X_idx_sorted,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 254, in fit\n",
            "    % self.min_samples_split\n",
            "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "180 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 942, in fit\n",
            "    X_idx_sorted=X_idx_sorted,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\", line 352, in fit\n",
            "    criterion = CRITERIA_CLF[self.criterion](\n",
            "KeyError: 'log_loss'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.43344772 0.43344772 0.43344772        nan 0.43344772\n",
            " 0.43344772 0.43344772        nan 0.43344772 0.43344772 0.43344772\n",
            "        nan 0.43344772 0.43344772 0.43344772        nan 0.42916953\n",
            " 0.42916953 0.42916953        nan 0.42916953 0.42916953 0.42916953\n",
            "        nan 0.42916953 0.42916953 0.42916953        nan 0.42916953\n",
            " 0.42916953 0.42916953        nan 0.4119881  0.40338595 0.40123542\n",
            "        nan 0.40981469 0.41411576 0.4162663         nan 0.41196523\n",
            " 0.40981469 0.41196523        nan 0.40981469 0.4119881  0.41413864\n",
            "        nan 0.46783345 0.46783345 0.46783345        nan 0.46783345\n",
            " 0.46783345 0.46783345        nan 0.46783345 0.46783345 0.46783345\n",
            "        nan 0.46783345 0.46783345 0.46783345        nan 0.45710364\n",
            " 0.45710364 0.45710364        nan 0.45925418 0.45925418 0.45710364\n",
            "        nan 0.45710364 0.45710364 0.45710364        nan 0.45710364\n",
            " 0.45710364 0.45710364        nan 0.42921528 0.43559826 0.43134294\n",
            "        nan 0.43562114 0.43132006 0.4291924         nan 0.43562114\n",
            " 0.43344772 0.43559826        nan 0.4377488  0.4334706  0.4334706\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'criterion': 'entropy',\n",
              " 'max_depth': 2,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf=DecisionTreeClassifier(criterion='entropy', max_depth=2,min_samples_leaf=1,min_samples_split=2)\n",
        "clf.fit(x_train, y_train)\n",
        "from sklearn.model_selection import cross_validate\n",
        "def cross_metrics(model, X_train, y_train):\n",
        "    scoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted']\n",
        "    scores = cross_validate(model, X_train, y_train, scoring=scoring, cv=3)\n",
        "    sorted(scores.keys())\n",
        "    LR_fit_time = scores['fit_time'].mean()\n",
        "    LR_score_time = scores['score_time'].mean()\n",
        "    LR_accuracy = scores['test_accuracy'].mean()\n",
        "    LR_precision = scores['test_precision_macro'].mean()\n",
        "    LR_recall = scores['test_recall_macro'].mean()\n",
        "    LR_f1 = scores['test_f1_weighted'].mean()\n",
        "    \n",
        "    print(\"fit_time: \",LR_fit_time)\n",
        "    print(\"score time: \",LR_score_time)\n",
        "    print(\"test_accuracy: \",LR_accuracy)\n",
        "    print(\"test_precision_macro: \",LR_precision)\n",
        "    print(\"test_recall_macro: \",LR_recall)\n",
        "    print(\"test_f1_weighted: \",LR_f1)\n",
        "cross_metrics(clf, x, y)"
      ],
      "metadata": {
        "id": "2TJT9O80u_hw",
        "outputId": "e347ef31-934f-4d85-b65e-c6283262802b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fit_time:  0.01503300666809082\n",
            "score time:  0.004897117614746094\n",
            "test_accuracy:  0.4419768403639372\n",
            "test_precision_macro:  0.17890200785831714\n",
            "test_recall_macro:  0.24023788102735474\n",
            "test_f1_weighted:  0.32846961602406016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7OCyEsCwk55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}